{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-with-nltk.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xprilion/introduction-to-nltk/blob/master/Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "c902b800de2095c8a86d923086f7e91d33113184",
        "id": "RpMlK3uLn03E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP with NLTK \n",
        "\n",
        "Welcome to a Natural Language Processing tutorial using NLTK.\n",
        "\n",
        "## What is Natural Language Processing(NLP) ?\n",
        "\n",
        "\n",
        "Let us understand the concept of NLP in detail\n",
        "\n",
        "Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.\n",
        "\n",
        "The study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.\n",
        "\n",
        "Before we get into details of NLP first let us try to answer the below question \n",
        "\n",
        "- What natural language is and how it is different from other types of data?\n",
        "\n",
        "Natural language refers to the way we, humans, communicate with each other namely speech and text.We are surrounded by text.\n",
        "Think about how much text you see each day:\n",
        "\n",
        "- Signs\n",
        "- Menus\n",
        "- Email\n",
        "- SMS\n",
        "- Web Pages\n",
        "\n",
        "and so much moreâ€¦\n",
        "\n",
        "The list is endless.\n",
        "\n",
        "Now think about speech.We may speak to each other, as a species, more than we write. It may even be easier to learn to speak than to write.Voice and text are how we communicate with each other.Given the importance of this type of data, we must have methods to understand and reason about natural language, just like we do for other types of data.\n",
        "\n",
        "Now lets get into details of this tutorial. "
      ]
    },
    {
      "metadata": {
        "_uuid": "8e0ef2d48277348836fb9c4432ed6048da6aa9d3",
        "id": "i8C7dxdMn03H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### <a id='01'>Natural Language Processing using NLTK</a>\n",
        "\n",
        "#### <a id='1'>1. Introduction to NLTK</a>\n",
        "\n",
        "The NLTK module is a massive tool kit, aimed at helping with the entire Natural Language Processing (NLP) methodology. NLTK will aid with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping machine to understand what the text is all about. In this tutorial, we're going to tackle the field of opinion mining, or sentiment analysis.\n",
        "\n",
        "In our path to learning how to do sentiment analysis with NLTK, we're going to learn the following:\n",
        "\n",
        "- Tokenizing - Splitting sentences and words from the body of text.\n",
        "- Part of Speech tagging\n",
        "- Machine Learning with the Naive Bayes classifier\n",
        "- How to tie in Scikit-learn (sklearn) with NLTK\n",
        "- Training classifiers with datasets\n",
        "- Performing live, streaming, sentiment analysis with Twitter.\n",
        "\n",
        "...and much more.\n",
        "\n",
        "In order to get started, you are going to need the NLTK module, as well as Python."
      ]
    },
    {
      "metadata": {
        "id": "e6JuUHjbpR0y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9554da5fb5b9fde48c12ecea076a97f11bc4cba8",
        "id": "fZ11ybCln03I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "405d7bb3-668d-429e-8680-008bbdec63af"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f12fe40a9c74a53eaa592dd7e01d37b1ace9feac",
        "id": "GPKj4O6Hn03N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='2'>2. Tokenizing Words & Sentences</a>\n",
        "\n",
        "Tokenization is the process of breaking up the given text into units called tokens. The tokens may be words or number or punctuation mark or even sentences. Tokenization does this task by locating word boundaries. Ending point of a word and beginning of the next word is called word boundaries. Tokenization is also known as word segmentation.\n",
        "\n",
        "- <b>Challenges in tokenization</b> depends on the type of language. Languages such as English and French are referred to as space-delimited as most of the words are separated from each other by white spaces. Languages such as Chinese and Thai are referred to as unsegmented as words do not have clear boundaries. Tokenising unsegmented language sentences requires additional lexical and morphological information. Tokenization is also affected by writing system and the typographical structure of the words. Structures of languges can be grouped into three categories:\n",
        "\n",
        "    - Isolating: Words do not divide into smaller units. Example: Mandarin Chinese\n",
        "\n",
        "    - Agglutinative: Words divide into smaller units. Example: Japanese, Tamil\n",
        "\n",
        "    - Inflectional: Boundaries between morphemes are not clear and ambiguous in terms of grammatical meaning. Example: Latin.\n",
        "\n",
        "\n",
        "Let us understand some more basic terminology.\n",
        "\n",
        "- What is Corpora?\n",
        "\n",
        "It is a body of text e.g Medical journal, Presidential speech, English language\n",
        "\n",
        "- What is Lexicon?\n",
        "\n",
        "Lexicon is nothing but words and their means .E.g Investor speak vs. Regular English speak\n",
        "\n",
        "i.e Investor talk about \"BULL\" as some stock going positive in the market which bullish as to the regular word of \"BULL\" describing the usual animal.\n",
        "\n",
        "    \n",
        "So in simple for now let us look at Word Tokenizer and Sentence Tokenizer using NLTK."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8bda89077120dc063d30d14a389223a4f22d7961",
        "id": "fRa0ZCTRn03O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "adf585b5aacb9fb961afd9bc12bf0fc5d0c56e44",
        "_kg_hide-input": true,
        "id": "MeGQPXbZn03T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "05b8968c-1816-4cdc-8e14-84f759606e2b"
      },
      "cell_type": "code",
      "source": [
        "example_text = \"Hi Mr.Pavan , How are you doing today? You got a nice job at IBM. Wow thats an awesome car. Weather is great.\"\n",
        "\n",
        "print(sent_tokenize(example_text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi Mr.Pavan , How are you doing today?', 'You got a nice job at IBM.', 'Wow thats an awesome car.', 'Weather is great.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "31304a934df65807ad390c6332e4f7f94d7c1ddc",
        "id": "_a-rrsmhn03W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see that sentence tokenizer did split the above example text into seperate sentences.Now let us look at word tokenizer below"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb6b7103d2d600b10c4e806d46897280b979ac91",
        "id": "CqQMfWmdn03W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "d348513c-de79-4adf-8fc0-55309ca175d8"
      },
      "cell_type": "code",
      "source": [
        "print(word_tokenize(example_text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi', 'Mr.Pavan', ',', 'How', 'are', 'you', 'doing', 'today', '?', 'You', 'got', 'a', 'nice', 'job', 'at', 'IBM', '.', 'Wow', 'thats', 'an', 'awesome', 'car', '.', 'Weather', 'is', 'great', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4b1d74555166094c64b248945a8ecc0d2446977e",
        "id": "5Htcl3w7n03a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see that word tokenizer did split the above example text into seperate words."
      ]
    },
    {
      "metadata": {
        "_uuid": "3b8d43a84027fd860b2ede056f23a7661a4ddbba",
        "id": "a9b8YvtFn03b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='3'>3. Stopwords</a>\n",
        "\n",
        "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n",
        "\n",
        "Basically during the pre processing of natural language text we eliminate the stopwords as they are redundant and do not convey any meaning insight in the data."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbbfc1a7c0b3a870dba5302b6c7804501ea6f0bb",
        "id": "jsAQ2oSan03c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1af5ee7dad30fef9a758d97bd415fc08480d75e9",
        "id": "xEE3VeUKn03f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let set the stopwords for english language.Let us see what are all the stopwords in english"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cce9c2e27c871128d3d0c7eabb59a3d52b4d3d95",
        "id": "A_TJ67qXn03g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "91aa7ded-4587-4944-9114-700dbff2425d"
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "print(stop_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'don', \"wasn't\", 'was', 'who', 'during', 'it', \"couldn't\", \"you've\", 'll', 'mustn', 'yourselves', 'each', 'will', 'or', \"hadn't\", 'against', \"shouldn't\", 'do', 'doesn', 've', 'are', 'just', 'yourself', 'should', 'the', 'shan', 'himself', 'whom', 'and', 'because', 'by', 'over', 'where', 'more', 'how', 'such', 'so', 'needn', 'been', 'same', 'my', \"doesn't\", \"you'd\", 'until', 'both', 'itself', 'own', 'our', 'he', 'before', 'too', 'between', 'did', 'off', 'does', 'further', 'only', \"shan't\", 'me', 'these', 'hasn', \"won't\", 'now', 'through', 'again', \"mightn't\", 'there', 'can', 'shouldn', 'his', 'a', 'why', 'wouldn', \"don't\", 'be', 'of', 'this', \"she's\", 'aren', 'hadn', 'them', 'after', 'your', 'y', 'up', 're', 'she', 'couldn', 'myself', 'for', 'nor', 'we', 'to', 'o', 'very', 'but', \"that'll\", 'some', \"didn't\", 'what', 'down', 'doing', 'm', 'd', 'those', 'isn', 'am', 'any', 'ain', \"you're\", 'about', 'other', 'ourselves', \"you'll\", 'were', 'herself', 'as', \"haven't\", \"wouldn't\", 'is', 'which', 'haven', 'then', 'all', 'into', \"mustn't\", 'than', 'most', 'they', 'yours', 'its', \"weren't\", 'have', 'i', 'having', 'hers', 'above', 'from', 'below', 'here', 'won', 'once', 'out', 'didn', \"aren't\", 'him', 'not', 'mightn', 'being', 'that', \"should've\", 'theirs', 'you', 'under', 't', 'few', 'her', 'while', 'when', 'at', \"needn't\", 'their', 'an', 'with', \"isn't\", 'if', \"hasn't\", 'wasn', 'in', 'ma', 'ours', \"it's\", 's', 'weren', 'themselves', 'has', 'no', 'had', 'on'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2e0fe0fb12753330bd180deafe380acbeaf1455c",
        "id": "ujFFiPJRn03k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let us tokenize the sample text and filter the sentence by removing the stopwords from it ."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fa808238a54f04c1315d099f22cc34526673aa7",
        "id": "V7LmcCCTn03l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "20eb816c-8f3b-41a0-ab25-8413a0173b1f"
      },
      "cell_type": "code",
      "source": [
        "example_text = \"Hi Mr.Pavan , How are you doing today?. Cool you got a nice job at IBM. Wow thats an awesome car. Weather is great.\"\n",
        "words = word_tokenize(example_text)\n",
        "filtered_sentence = []\n",
        "for w in words:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "print(filtered_sentence)    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi', 'Mr.Pavan', ',', 'How', 'today', '?', '.', 'Cool', 'got', 'nice', 'job', 'IBM', '.', 'Wow', 'thats', 'awesome', 'car', '.', 'Weather', 'great', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9021770a8f9670b94cd00f3278867d7cd2a7821b",
        "id": "ngm6Qf48n03p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see from above thats how we can filter out the stopwords from a given content and further process the data ."
      ]
    },
    {
      "metadata": {
        "_uuid": "4665a815827acd54856b9067311e924dd42a6dfc",
        "id": "GcqLODGZn03q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='4'>4. Stemming Words</a>\n",
        "\n",
        "Stemming is the process of reducing infected or derived words to their word stem,base or root form. It basically affixes to suffixes and prefixes or to the roots of words known as a lemma.It is also a preprocessing step in natural language processing.\n",
        "\n",
        "Examples: Words like\n",
        "- organise, organising ,organisation the root of its stem is organis.\n",
        "- intelligence,intelligently the root of its stem is intelligen\n",
        "\n",
        "So stemming produces intermediate representation of the word which may not have any meaning.In this case \"intelligen\" has no meaning.\n",
        "\n",
        "The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n",
        "\n",
        "The reason why we stem is to shorten the lookup, and normalize sentences.\n",
        "\n",
        "One of the most popular stemming algorithms is the Porter stemmer, which has been around since 1979.\n",
        "\n",
        "First, we're going to grab and define our stemmer:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "596c86a0377095d5bf56d0032f71b46c8ffce043",
        "id": "OAFvHYyYn03r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5fe8341cc376c10e8459beb498d6aacdb1d15abd",
        "id": "cikEHz1Fn03v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "aa7b98cc-366f-4622-bc5a-aa61d1811b10"
      },
      "cell_type": "code",
      "source": [
        "txt = \"John is an intelligent individual.He intelligently does smart work. He is a top performer in the company.\"\n",
        "sentences = sent_tokenize(txt)\n",
        "stemmer = PorterStemmer()\n",
        "new_sentence = []\n",
        "for i in range(len(sentences)):\n",
        "    words = word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    new_sentence.append(' '.join(words))\n",
        "print(new_sentence)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['john is an intellig individual.h intellig doe smart work .', 'He is a top perform in the compani .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "2bde47d7f80f643c42f32ac068bd03e0fd734f3e",
        "id": "iKDSstMWn03y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see above the word \"intellig\" and it confirms that stemming process is complete. Now let us look at Lemmatization"
      ]
    },
    {
      "metadata": {
        "_uuid": "7d2fd4fedec70645eeb47627ff21b80b6513245d",
        "id": "QZYrDHNVn03z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='5'>5. Lemmatization</a>\n",
        "\n",
        "It is same as stemming process but the intermediate representation/root has a meaning.It is also a preprocessing step in natural language processing.\n",
        "\n",
        "Examples: Words like\n",
        "- going ,goes,gone - when we do lemmatization we get \"go\" \n",
        "- intelligence,intelligently - when we do lemmatization we get \"intelligent\".\n",
        "\n",
        "So lemmatization produces intermediate representation of the word which has a meaning.In this case \"intelligent\" has meaning."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37966e4ac6c807007285c01079c7310f22c36351",
        "id": "pSOV3CINn030",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a30461e0972d50a288b4b862e9bae11815a8f82c",
        "id": "SAVhD_won033",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8d747d20-6cbf-436e-acbe-3eb84f96e969"
      },
      "cell_type": "code",
      "source": [
        "txt = \"John is an intelligent individual.He intelligently does smart work. He is a top performer in the company.\"\n",
        "sentences = sent_tokenize(txt)\n",
        "lemmtizer = WordNetLemmatizer()\n",
        "new__lemmatize_sentence = []\n",
        "for i in range(len(sentences)):\n",
        "    words = word_tokenize(sentences[i])\n",
        "    words = [lemmtizer.lemmatize(word) for word in words]\n",
        "    new__lemmatize_sentence.append(' '.join(words))\n",
        "print(new__lemmatize_sentence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['John is an intelligent individual.He intelligently doe smart work .', 'He is a top performer in the company .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a89e799efeadaffc210d92958bc7a3637239dc89",
        "id": "-ySCkDx4n037",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='6'>6. Part of Speech Tagging</a>\n",
        "\n",
        "One of the more powerful aspects of the NLTK is the Part of Speech tagging that it can do. This means labeling words in a sentence as nouns, adjectives, verbs...etc. Even more impressive, it also labels by tense, and more. Here's a list of the tags, what they mean, and some examples:\n",
        "\n",
        "##### POS tag list:\n",
        "\n",
        "- CC\tcoordinating conjunction\n",
        "- CD\tcardinal digit\n",
        "- DT\tdeterminer\n",
        "- EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
        "- FW\tforeign word\n",
        "- IN\tpreposition/subordinating conjunction\n",
        "- JJ\tadjective\t'big'\n",
        "- JJR\tadjective, comparative\t'bigger'\n",
        "- JJS\tadjective, superlative\t'biggest'\n",
        "- LS\tlist marker\t1)\n",
        "- MD\tmodal\tcould, will\n",
        "- NN\tnoun, singular 'desk'\n",
        "- NNS\tnoun plural\t'desks'\n",
        "- NNP\tproper noun, singular\t'Harrison'\n",
        "- NNPS\tproper noun, plural\t'Americans'\n",
        "- PDT\tpredeterminer\t'all the kids'\n",
        "- POS\tpossessive ending\tparent\\'s\n",
        "- PRP\tpersonal pronoun\tI, he, she\n",
        "- PRPdollar\tpossessive pronoun\tmy, his, hers\n",
        "- RB\tadverb\tvery, silently,\n",
        "- RBR\tadverb, comparative\tbetter\n",
        "- RBS\tadverb, superlative\tbest\n",
        "- RP\tparticle\tgive up\n",
        "- TO\tto\tgo 'to' the store.\n",
        "- UH\tinterjection\terrrrrrrrm\n",
        "- VB\tverb, base form\ttake\n",
        "- VBD\tverb, past tense\ttook\n",
        "- VBG\tverb, gerund/present participle\ttaking\n",
        "- VBN\tverb, past participle\ttaken\n",
        "- VBP\tverb, sing. present, non-3d\ttake\n",
        "- VBZ\tverb, 3rd person sing. present\ttakes\n",
        "- WDT\twh-determiner\twhich\n",
        "- WP\twh-pronoun\twho, what\n",
        "- WPdollar\tpossessive wh-pronoun\twhose\n",
        "- WRB\twh-abverb\twhere, when\n",
        "\n",
        " Now let us use a  new sentence tokenizer, called the PunktSentenceTokenizer. This tokenizer is capable of unsupervised machine learning, so we can actually train it on any body of text that we use."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "040b63f6dabf4530c64c4223b7bb3cf5c9b50838",
        "id": "mH-HJUu9n038",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "# Now, let's create our training and testing data:\n",
        "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
        "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
        "# Next, we can train the Punkt tokenizer like:\n",
        "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
        "# Then we can actually tokenize, using:\n",
        "tokenized = cust_tokenizer.tokenize(sample_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "758868c2e960c04578f5e95179b4e61d7ab5b452",
        "id": "eoRtNAm0n04A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can finish up this part of speech tagging script by creating a function that will run through and tag all of the parts of speech per sentence like so:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea9dfa6cf53be515c5923d88f93e87fa3d5852a9",
        "id": "A0b7mDtRn04B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "5c40da45-2760-4d87-a356-3b7c44739524"
      },
      "cell_type": "code",
      "source": [
        "print(\"Speech Tagging Output\")\n",
        "def process_text():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            print(tagged)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "process_text()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Speech Tagging Output\n",
            "[('Crocodiles', 'NNS'), ('are', 'VBP'), ('large', 'JJ'), ('aquatic', 'JJ'), ('reptiles', 'NNS'), ('which', 'WDT'), ('are', 'VBP'), ('carnivorous.Allegators', 'NNS'), ('belong', 'RB'), ('to', 'TO'), ('this', 'DT'), ('same', 'JJ'), ('reptile', 'NN'), ('species', 'NNS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "21a0b53a3a3fc76d7b587ef45eb1c2e655d246bc",
        "id": "1Zu6DERin04E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='7'>7. Chunking</a>\n",
        "\n",
        "Now that we know the parts of speech, we can do what is called chunking, and group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them.\n",
        "\n",
        "In order to chunk, we combine the part of speech tags with regular expressions. Mainly from regular expressions, we are going to utilize the following:\n",
        "\n",
        "\"+\" = match 1 or more\n",
        "\n",
        "\"?\" = match 0 or 1 repetitions.\n",
        "\n",
        "\"*\" = match 0 or MORE repetitions\t  \n",
        "\n",
        "\".\" = Any character except a new line\n",
        "\n",
        "The last things to note is that the part of speech tags are denoted with the \"<\" and \">\" and we can also place regular expressions within the tags themselves, so account for things like \"all nouns\" (<N.*>)\n",
        "\n",
        "Let us take the same code from the above Speech Tagging section and modify it to include chunking for noun plural (NNS) and adjective (JJ)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de9380cea9e1d9a803fa4eb7f704960d4ec0f3d1",
        "id": "CUmLoJWFn04F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "09a02329-a432-413b-e7e6-c3f3c3dbaa88"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# Now, let's create our training and testing data:\n",
        "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
        "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
        "\n",
        "# Next, we can train the Punkt tokenizer like:\n",
        "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
        "\n",
        "# Then we can actually tokenize, using:\n",
        "\n",
        "tokenized = cust_tokenizer.tokenize(sample_text)\n",
        "print(\"Chunked Output\")\n",
        "def process_text():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            chunkGram = r\"\"\"Chunk:{<NNS.?>*<JJ>+}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            #chunked.draw()\n",
        "            print(chunked)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "process_text()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chunked Output\n",
            "(S\n",
            "  Crocodiles/NNS\n",
            "  are/VBP\n",
            "  (Chunk large/JJ aquatic/JJ)\n",
            "  reptiles/NNS\n",
            "  which/WDT\n",
            "  are/VBP\n",
            "  carnivorous.Allegators/NNS\n",
            "  belong/RB\n",
            "  to/TO\n",
            "  this/DT\n",
            "  (Chunk same/JJ)\n",
            "  reptile/NN\n",
            "  species/NNS)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "d8c37bf1a24d25c79701fa5030c0858b9de28f60",
        "id": "XL78tXhfn04K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='8'>8. Chinking</a>\n",
        "\n",
        "You may find that, after a lot of chunking, you have some words in your chunk you still do not want, but you have no idea how to get rid of them by chunking. You may find that chinking is your solution.\n",
        "\n",
        "Chinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink.\n",
        "\n",
        "The code is very similar, you just denote the chink, after the chunk, with }{ instead of the chunk's {}"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bcbef154f554dd89aa7096933fb4d97680979d7",
        "id": "w2DoSvAun04M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "11152473-0505-417c-8ca3-f1613676fe3c"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# Now, let's create our training and testing data:\n",
        "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
        "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
        "\n",
        "# Next, we can train the Punkt tokenizer like:\n",
        "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
        "\n",
        "# Then we can actually tokenize, using:\n",
        "\n",
        "tokenized = cust_tokenizer.tokenize(sample_text)\n",
        "\n",
        "print(\"Chinked Output\")\n",
        "def process_text():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
        "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            #chunked.draw()\n",
        "            print(chunked)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "process_text()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chinked Output\n",
            "(S\n",
            "  (Chunk Crocodiles/NNS)\n",
            "  are/VBP\n",
            "  (Chunk large/JJ aquatic/JJ reptiles/NNS which/WDT)\n",
            "  are/VBP\n",
            "  (Chunk carnivorous.Allegators/NNS belong/RB)\n",
            "  to/TO\n",
            "  this/DT\n",
            "  (Chunk same/JJ reptile/NN species/NNS))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ec61539149184ce912d467c64981e30c615a9cbc",
        "id": "XAsEFjyVn04S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='9'>9. Named Entity Recognition</a>\n",
        "\n",
        "One of the most major forms of chunking in natural language processing is called \"Named Entity Recognition.\" The idea is to have the machine immediately be able to pull out \"entities\" like people, places, things, locations, monetary figures, and more.\n",
        "\n",
        "This can be a bit of a challenge, but NLTK is this built in for us. There are two major options with NLTK's named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d162df34e1de5034e78b937b93a26e8e95ea932",
        "id": "RzfFDOuzn04S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "af8cfb76-37d0-4271-f22f-c000f4364a33"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# Now, let's create our training and testing data:\n",
        "train_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\n",
        "sample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n",
        "\n",
        "# Next, we can train the Punkt tokenizer like:\n",
        "cust_tokenizer = PunktSentenceTokenizer(train_txt)\n",
        "\n",
        "# Then we can actually tokenize, using:\n",
        "\n",
        "tokenized = cust_tokenizer.tokenize(sample_text)\n",
        "\n",
        "print(\"Named Entity Output\")\n",
        "def process_text():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            namedEnt = nltk.ne_chunk(tagged,binary = True)\n",
        "            print(namedEnt)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "process_text()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Named Entity Output\n",
            "(S\n",
            "  Crocodiles/NNS\n",
            "  are/VBP\n",
            "  large/JJ\n",
            "  aquatic/JJ\n",
            "  reptiles/NNS\n",
            "  which/WDT\n",
            "  are/VBP\n",
            "  carnivorous.Allegators/NNS\n",
            "  belong/RB\n",
            "  to/TO\n",
            "  this/DT\n",
            "  same/JJ\n",
            "  reptile/NN\n",
            "  species/NNS)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "97e20cdfa3b6d1af30ebb1e4fbe5b8e68e8d7902",
        "id": "-3WhYLUan04V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### <a id='10'>10. The Corpora</a>\n",
        "\n",
        "The NLTK corpus is a massive dump of all kinds of natural language data sets that are definitely worth taking a look at.\n",
        "\n",
        "Almost all of the files in the NLTK corpus follow the same rules for accessing them by using the NLTK module, but nothing is magical about them. These files are plain text files for the most part, some are XML and some are other formats, but they are all accessible by manual, or via the module and Python. Let's talk about viewing them manually."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02dca3ddfdf26e5245b3d2c277f2e643ac55382a",
        "id": "_M7NmAsZn04W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "ed098704-bdae-459f-c218-fbc80f92c6db"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
        "tok = sent_tokenize(sample)\n",
        "print(tok[5:15])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ebJWT11sppWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}